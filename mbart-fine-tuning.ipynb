{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":10558587,"sourceType":"datasetVersion","datasetId":6532414}],"dockerImageVersionId":30840,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install sacrebleu","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-31T12:07:29.382623Z","iopub.execute_input":"2025-01-31T12:07:29.382907Z","iopub.status.idle":"2025-01-31T12:07:34.070969Z","shell.execute_reply.started":"2025-01-31T12:07:29.382885Z","shell.execute_reply":"2025-01-31T12:07:34.070134Z"}},"outputs":[{"name":"stdout","text":"Collecting sacrebleu\n  Downloading sacrebleu-2.5.1-py3-none-any.whl.metadata (51 kB)\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m51.8/51.8 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting portalocker (from sacrebleu)\n  Downloading portalocker-3.1.1-py3-none-any.whl.metadata (8.6 kB)\nRequirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (2024.11.6)\nRequirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (0.9.0)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (1.26.4)\nRequirement already satisfied: colorama in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (0.4.6)\nRequirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (5.3.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->sacrebleu) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->sacrebleu) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->sacrebleu) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->sacrebleu) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->sacrebleu) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->sacrebleu) (2.4.1)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->sacrebleu) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->sacrebleu) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->sacrebleu) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.17->sacrebleu) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.17->sacrebleu) (2024.2.0)\nDownloading sacrebleu-2.5.1-py3-none-any.whl (104 kB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m104.1/104.1 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading portalocker-3.1.1-py3-none-any.whl (19 kB)\nInstalling collected packages: portalocker, sacrebleu\nSuccessfully installed portalocker-3.1.1 sacrebleu-2.5.1\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"!pip install --upgrade transformers","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-31T12:07:37.337082Z","iopub.execute_input":"2025-01-31T12:07:37.337361Z","iopub.status.idle":"2025-01-31T12:07:48.368847Z","shell.execute_reply.started":"2025-01-31T12:07:37.337336Z","shell.execute_reply":"2025-01-31T12:07:48.367962Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.47.0)\nCollecting transformers\n  Downloading transformers-4.48.2-py3-none-any.whl.metadata (44 kB)\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.27.0)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.11.6)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.21.0)\nRequirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.67.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (2024.9.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (4.12.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (2.4.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.12.14)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->transformers) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->transformers) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.17->transformers) (2024.2.0)\nDownloading transformers-4.48.2-py3-none-any.whl (9.7 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m9.7/9.7 MB\u001b[0m \u001b[31m81.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: transformers\n  Attempting uninstall: transformers\n    Found existing installation: transformers 4.47.0\n    Uninstalling transformers-4.47.0:\n      Successfully uninstalled transformers-4.47.0\nSuccessfully installed transformers-4.48.2\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# Cell 1: Imports\nimport pandas as pd\nimport numpy as np\nimport torch\nfrom datasets import Dataset, DatasetDict\nfrom transformers import (\n    AutoTokenizer, \n    AutoModelForSeq2SeqLM, \n    Seq2SeqTrainingArguments, \n    Seq2SeqTrainer,\n    DataCollatorForSeq2Seq\n)\nimport sacrebleu\nfrom sklearn.model_selection import train_test_split","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-01-31T12:07:54.167333Z","iopub.execute_input":"2025-01-31T12:07:54.167689Z","iopub.status.idle":"2025-01-31T12:08:15.316683Z","shell.execute_reply.started":"2025-01-31T12:07:54.167658Z","shell.execute_reply":"2025-01-31T12:08:15.315957Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"# Charger et prÃ©processer les donnÃ©es\ndf = pd.read_csv('/kaggle/input/cleaned-darija-dataset/cleaned_darija_dataset.csv')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-31T12:08:20.225304Z","iopub.execute_input":"2025-01-31T12:08:20.226018Z","iopub.status.idle":"2025-01-31T12:08:20.448746Z","shell.execute_reply.started":"2025-01-31T12:08:20.225983Z","shell.execute_reply":"2025-01-31T12:08:20.448007Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"# Split the data into train, validation, and test sets\ntrain_texts, temp_texts, train_translations, temp_translations = train_test_split(\n    df['darija'], df['english'], test_size=0.2, random_state=42\n)\n\nval_texts, test_texts, val_translations, test_translations = train_test_split(\n    temp_texts, temp_translations, test_size=0.5, random_state=42\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-31T12:08:22.731450Z","iopub.execute_input":"2025-01-31T12:08:22.731826Z","iopub.status.idle":"2025-01-31T12:08:22.759383Z","shell.execute_reply.started":"2025-01-31T12:08:22.731798Z","shell.execute_reply":"2025-01-31T12:08:22.758754Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"# Create datasets\ntrain_dataset = Dataset.from_dict({\n    'source': train_texts.tolist(),\n    'target': train_translations.tolist()\n})\nval_dataset = Dataset.from_dict({\n    'source': val_texts.tolist(),\n    'target': val_translations.tolist()\n})\ntest_dataset = Dataset.from_dict({\n    'source': test_texts.tolist(),\n    'target': test_translations.tolist()\n})\n\n# Combine into DatasetDict\ndataset = DatasetDict({\n    'train': train_dataset,\n    'validation': val_dataset,\n    'test': test_dataset\n})","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-31T12:08:24.869498Z","iopub.execute_input":"2025-01-31T12:08:24.869817Z","iopub.status.idle":"2025-01-31T12:08:24.977502Z","shell.execute_reply.started":"2025-01-31T12:08:24.869794Z","shell.execute_reply":"2025-01-31T12:08:24.976646Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"# Cell 3: Tokenization\nmodel_checkpoint = \"facebook/mbart-large-cc25\"\ntokenizer = AutoTokenizer.from_pretrained(model_checkpoint)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-31T12:08:26.996964Z","iopub.execute_input":"2025-01-31T12:08:26.997275Z","iopub.status.idle":"2025-01-31T12:08:30.178310Z","shell.execute_reply.started":"2025-01-31T12:08:26.997248Z","shell.execute_reply":"2025-01-31T12:08:30.177653Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.19k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"55a1978361214d5dadcf5c8118ab6281"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4c5cbce5a41740e99da0c36b035a1bdb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/9.10M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3321a93276f14dd3ad8d285633e610df"}},"metadata":{}}],"execution_count":7},{"cell_type":"code","source":"# Set source and target language codes\ntokenizer.src_lang = \"en_XX\"  # English\ntokenizer.tgt_lang = \"ar_AR\"  # Arabic (adjust if needed)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-31T12:08:33.612289Z","iopub.execute_input":"2025-01-31T12:08:33.612660Z","iopub.status.idle":"2025-01-31T12:08:33.616600Z","shell.execute_reply.started":"2025-01-31T12:08:33.612631Z","shell.execute_reply":"2025-01-31T12:08:33.615800Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"def preprocess_function(examples):\n    inputs = examples['source']\n    targets = examples['target']\n    \n    model_inputs = tokenizer(\n        inputs, \n        max_length=128, \n        truncation=True, \n        padding='max_length'\n    )\n    \n    with tokenizer.as_target_tokenizer():\n        labels = tokenizer(\n            targets, \n            max_length=128, \n            truncation=True, \n            padding='max_length'\n        )\n    \n    model_inputs['labels'] = labels['input_ids']\n    return model_inputs\n\ntokenized_datasets = dataset.map(\n    preprocess_function, \n    batched=True, \n    remove_columns=dataset['train'].column_names\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-31T12:08:44.395881Z","iopub.execute_input":"2025-01-31T12:08:44.396190Z","iopub.status.idle":"2025-01-31T12:08:55.450696Z","shell.execute_reply.started":"2025-01-31T12:08:44.396165Z","shell.execute_reply":"2025-01-31T12:08:55.449729Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/75037 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7fb63d8590b1405fa6048d3c4bccb3a3"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:3961: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/9380 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b9ee612e6c98498ba0acc5a0ec66832d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/9380 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"201b698627e14d64bcd22ac2d079e4e8"}},"metadata":{}}],"execution_count":9},{"cell_type":"code","source":"# Cell 4: Model Preparation\nmodel = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)\n\ndata_collator = DataCollatorForSeq2Seq(\n    tokenizer, \n    model=model, \n    return_tensors=\"pt\"\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-31T12:09:00.307555Z","iopub.execute_input":"2025-01-31T12:09:00.307924Z","iopub.status.idle":"2025-01-31T12:10:31.076214Z","shell.execute_reply.started":"2025-01-31T12:09:00.307897Z","shell.execute_reply":"2025-01-31T12:10:31.075233Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/2.44G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6fd06b9ae62d4b3797f52320b730df15"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/2.44G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d34b2483de064638acd686d3d8fdd356"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/205 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4f8b9b7373ad48dbba754b7a46bff6cf"}},"metadata":{}}],"execution_count":10},{"cell_type":"code","source":"# Cell 5: Evaluation Metric (BLEU Score)\ndef compute_bleu(eval_preds):\n    preds, labels = eval_preds\n    \n    if isinstance(preds, tuple):\n        preds = preds[0]\n    \n    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n    \n    bleu_scores = [\n        sacrebleu.sentence_bleu(pred, [ref]).score \n        for pred, ref in zip(decoded_preds, decoded_labels)\n    ]\n    \n    return {\n        'bleu_score': np.mean(bleu_scores)\n    }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-31T12:10:44.598979Z","iopub.execute_input":"2025-01-31T12:10:44.599311Z","iopub.status.idle":"2025-01-31T12:10:44.604170Z","shell.execute_reply.started":"2025-01-31T12:10:44.599281Z","shell.execute_reply":"2025-01-31T12:10:44.603296Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"batch_size = 8\ngradient_accumulation_step = 2 #needs to be a batch size that can fit into memory \n\n\ntraining_args = Seq2SeqTrainingArguments(\n    output_dir=\"/kaggle/working/mbart_darija_translation\",\n    num_train_epochs=1,\n    learning_rate=5e-4,\n    gradient_accumulation_steps = gradient_accumulation_step, \n    per_device_train_batch_size=8,\n    per_device_eval_batch_size=8,\n    warmup_steps=500,\n    weight_decay=0.01,\n    logging_steps=10,\n    evaluation_strategy=\"steps\",\n    eval_steps=500,  # Moins d'Ã©valuations\n    save_strategy=\"no\",\n    #save_steps=500,  # Moins de sauvegardes\n    #save_total_limit=1,\n    load_best_model_at_end=False,\n    metric_for_best_model=\"bleu_score\",\n    push_to_hub=False,\n    fp16=False,  # Activation demi-prÃ©cision\n    predict_with_generate=True,  # DÃ©sactivation de la gÃ©nÃ©ration\n    generation_max_length=64,  # Limitation des sÃ©quences gÃ©nÃ©rÃ©es\n    #gradient_accumulation_steps=4,  # Accumulates gradients for 4 steps before updating weights.\n    report_to = \"none\",\n    gradient_checkpointing=True\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-31T12:10:46.991253Z","iopub.execute_input":"2025-01-31T12:10:46.991582Z","iopub.status.idle":"2025-01-31T12:10:47.156457Z","shell.execute_reply.started":"2025-01-31T12:10:46.991541Z","shell.execute_reply":"2025-01-31T12:10:47.155656Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ğŸ¤— Transformers. Use `eval_strategy` instead\n  warnings.warn(\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"# Cell 7: Trainer\ntrainer = Seq2SeqTrainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_datasets['train'],\n    eval_dataset=tokenized_datasets['validation'],\n    data_collator=data_collator,\n    processing_class=tokenizer,\n    compute_metrics=compute_bleu\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-31T12:10:50.608155Z","iopub.execute_input":"2025-01-31T12:10:50.608443Z","iopub.status.idle":"2025-01-31T12:10:51.516782Z","shell.execute_reply.started":"2025-01-31T12:10:50.608421Z","shell.execute_reply":"2025-01-31T12:10:51.516083Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"import os\nos.environ[\"WANDB_MODE\"] = \"disabled\"\nos.environ[\"WANDB_DISABLED\"] = \"true\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-31T12:10:54.348405Z","iopub.execute_input":"2025-01-31T12:10:54.348750Z","iopub.status.idle":"2025-01-31T12:10:54.352702Z","shell.execute_reply.started":"2025-01-31T12:10:54.348723Z","shell.execute_reply":"2025-01-31T12:10:54.351821Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"# Cell 8: Training\n# Train the model\n\ntrainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-31T12:10:57.146154Z","iopub.execute_input":"2025-01-31T12:10:57.146464Z","execution_failed":"2025-01-31T16:16:49.091Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='3011' max='4690' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [3011/4690 2:27:28 < 1:22:17, 0.34 it/s, Epoch 0.64/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Bleu Score</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>500</td>\n      <td>0.215000</td>\n      <td>0.197830</td>\n      <td>2.910495</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>0.169800</td>\n      <td>0.158156</td>\n      <td>8.946827</td>\n    </tr>\n    <tr>\n      <td>1500</td>\n      <td>0.159900</td>\n      <td>0.154144</td>\n      <td>11.077651</td>\n    </tr>\n    <tr>\n      <td>2000</td>\n      <td>0.151200</td>\n      <td>0.145760</td>\n      <td>14.141069</td>\n    </tr>\n    <tr>\n      <td>2500</td>\n      <td>0.131000</td>\n      <td>0.137811</td>\n      <td>16.755393</td>\n    </tr>\n    <tr>\n      <td>3000</td>\n      <td>0.137900</td>\n      <td>0.131435</td>\n      <td>20.756764</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}}],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\ntrain_loss = [entry['loss'] for entry in trainer.state.log_history if 'loss' in entry]\nepochs = [entry['epoch'] for entry in trainer.state.log_history if 'loss' in entry]\n\neval_loss = [entry['eval_loss'] for entry in trainer.state.log_history if 'eval_loss' in entry]\neval_epochs = [entry['epoch'] for entry in trainer.state.log_history if 'eval_loss' in entry]\n\nplt.plot(epochs, train_loss, label=\"Training Loss\")\nplt.plot(eval_epochs, eval_loss, label=\"Validation Loss\")\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Loss\")\nplt.title(\"Training and Validation Loss\")\nplt.legend()\nplt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Cell 9: Final Evaluation\ntest_results = trainer.evaluate(\n    eval_dataset=tokenized_datasets['test'], \n    metric_key_prefix=\"test\"\n)\nprint(\"Test Results:\", test_results)\n","metadata":{"trusted":true,"execution":{"execution_failed":"2025-01-27T12:35:32.195Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Optional: Save the model\ntrainer.save_model(\"./final_mbert_darija_translation_model\")","metadata":{"trusted":true,"execution":{"execution_failed":"2025-01-27T12:35:32.196Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Cell 10: Prediction Function\ndef translate_text(text, model, tokenizer, max_length=128):\n    inputs = tokenizer(\n        text, \n        return_tensors=\"pt\", \n        max_length=max_length, \n        truncation=True, \n        padding=True\n    )\n    \n    # Specify target language for generation\n    inputs[\"decoder_start_token_id\"] = tokenizer.lang_code_to_id[\"ar_AR\"]\n    \n    outputs = model.generate(\n        inputs['input_ids'], \n        max_length=max_length, \n        num_beams=4, \n        early_stopping=True\n    )\n    \n    translation = tokenizer.decode(\n        outputs[0], \n        skip_special_tokens=True\n    )\n    \n    return translation\n\n# Load saved model and tokenizer\nsaved_model = AutoModelForSeq2SeqLM.from_pretrained(\"./final_mbart_darija_translation_model\")\nsaved_tokenizer = AutoTokenizer.from_pretrained(\"./final_mbart_darija_translation_model\")\n\n# Example translations\nexample_texts = [\n    \"who are you\",\n    \"can you help me\"\n]\n\nfor text in example_texts:\n    translation = translate_text(text, saved_model, saved_tokenizer)\n    print(f\"Original: {text}\")\n    print(f\"Translation: {translation}\")\n    print(\"-\" * 50)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-01-27T12:35:32.196Z"}},"outputs":[],"execution_count":null}]}